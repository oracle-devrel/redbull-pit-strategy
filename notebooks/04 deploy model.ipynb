{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5fe2fe",
   "metadata": {},
   "source": [
    "***\n",
    "<p style=\"font-size:26px;\">\n",
    "<font color=red> Deploy and Invoke your model through Data Science Model Deployment</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle Cloud Infrastructure</font></p>\n",
    "\n",
    "***\n",
    "\n",
    "## Overview:\n",
    "Load pre-trained model for and deploy it.\n",
    "\n",
    "\n",
    "We demonstrate:\n",
    "1. Load saved model from previous notebook\n",
    "2. prepare and deploy the model using ADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c29cbf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Contents:\n",
    "\n",
    "* <a href='#data'>Load data</a>\n",
    "* <a href='#loadModel'>Load Model</a>\n",
    "* <a href='#serialize'>Serialization and model deploymnet </a>\n",
    "* <a href='#invoke_endpoint'>Invoke endpoint</a>\n",
    "* <a href='#cleanup'>Clean up and delete deployed model</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c7e26",
   "metadata": {},
   "source": [
    "### Load basic packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97da4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/datascience/WorkSpace/RedBull-Racining-TimeToPit/notebooks'\n",
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46de8462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(path)\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6d0bd",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "# Load data \n",
    "\n",
    "1. Load data `final_data.csv`\n",
    "2. Filter out races with lesser than 5 or higher than 50 laps\n",
    "2. Bucketize the target variable (StintLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d09bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908950b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------- \n",
      " Size of Data:  (1245, 29) \n",
      " -------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_path+'final_data.csv',)\n",
    "\n",
    "## remove any StintLen lesser than 5 and greather than 35\n",
    "## SintNumber==1\n",
    "df = df[(df['StintLen']>5) & (df['StintLen']<=35) & (df['Stint']==1)] #& (df['Stint']==1)\n",
    "\n",
    "\n",
    "## bucketize `target variable` and assign a label to each bucket\n",
    "bins = np.array([5,10,15,20,25,30,35])\n",
    "labels = np.arange(len(bins)-1)\n",
    "\n",
    "df['classLabels'] = pd.cut(df.StintLen, bins=bins,labels=labels)\n",
    "\n",
    "print('-'*80,'\\n', 'Size of Data: ', df.shape,'\\n', '-'*80,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5b72c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['EventName','Compound','Driver', 'TyreAge', \n",
    "           'meanAirTemp', 'meanTrackTemp', 'meanHumid', 'Rainfall', \n",
    "           'GridPosition', 'Position','CircuitLength','designedLaps','classLabels'] ].reset_index(drop=True)\n",
    "categorical_cols = []\n",
    "numerical_cols = []\n",
    "\n",
    "\n",
    "y = x[0:10].classLabels #pd.Categorical(x.classLabels).codes\n",
    "x = x[0:10].drop(['classLabels'],axis=1)\n",
    "\n",
    "# X_train_xgbc, X_test_xgbc, y_train_xgbc, y_test_xgbc = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "# print('Regression data size for train and test: ', len(X_train_xgbc), len(X_test_xgbc),\n",
    "#       '\\nClassification data size for train and test: ', len(X_train_xgbc), len(y_test_xgbc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6705ffb",
   "metadata": {},
   "source": [
    "<a id='loadModel'></a>\n",
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8660b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "ppl_model = joblib.load(data_path+'gbm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f49e64",
   "metadata": {},
   "source": [
    "<a id='serialize'></a>\n",
    "# Serialization and Model Deployment\n",
    "\n",
    "The Sklearn framework makes it easy to deploy a scikit-learn model into production. The `SklearnModel()` constructor takes a scikit-learn model and converts it into an `SklearnModel` object. To deploy the model into production, you need to prepare the model artifact, verify that the artifact works, save the model to the model catalog, and then deploy it.\n",
    "\n",
    "ADS provides a number of methods that greatly simplify the model deployment process. It also provides the `.summary_status()` method that outputs a dataframe that defines the steps, status, and detailed information about each step.Â \n",
    "\n",
    "<a id='serialize_sklearnmodel'></a>\n",
    "## Create a SklearnModel\n",
    "\n",
    "The `SklearnModel()` constructor takes a scikit-learn model along with the path that you want to use to store the model artifacts. An `SklearnModel` object is returned, and it is used to manage the deployment.\n",
    "\n",
    "The next cell creates a model artifact directory. This directory is used to store the artifacts that are needed to deploy the model. It also creates the `SklearnModel` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a99cb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/datascience/WorkSpace/RedBull-Racining-TimeToPit/notebooks'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c72fdd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from ads.model.framework.sklearn_model import SklearnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fea71ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact director: ../md-rbc-Pipeline-model\n"
     ]
    }
   ],
   "source": [
    "artifact_dir = f\"../md-rbc-Pipeline-model\"#tempfile.mkdtemp()\n",
    "print(f\"Model artifact director: {artifact_dir}\")\n",
    "sklearn_model = SklearnModel(estimator=ppl_model, artifact_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79834b04",
   "metadata": {},
   "source": [
    "The `.summary_status()` method of the `SklearnModel` class is a handy method to keep track of the progress that you are making in deploying the model. It creates a dataframe that lists the deployment steps, thier status, and details about them. The next cell returns the summary status dataframe. It shows that the initiate step has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4de6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.summary_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76b1b6",
   "metadata": {},
   "source": [
    "<a id='serialize_prepare'></a>\n",
    "## Prepare\n",
    "\n",
    "The prepare step is performed by the `.prepare()` method of the `SklearnModel` class. It creates a number of customized files that are used to run the model once it is deployed. These include:\n",
    "\n",
    "* `input_schema.json`: A JSON file that defines the nature of the features of the `X_sample` data. It includes metadata such as the data type, name, constraints, summary statistics, feature type, and more.\n",
    "* `model.joblib`: This is the default filename of the serialized model. It can be changed with the `model_file_name` attribute. By default, the model is stored in a joblib file. The parameter `as_onnx` can be used to save it in the ONNX format.\n",
    "* `output_schema.json`: A JSON file that defines the nature of the dependent variable in the `y_sample` data. It includes metadata such as the data type, name, constraints, summary statistics, feature type, and more.\n",
    "* `runtime.yaml`: This file contains information that is needed to set up the runtime environment on the deployment server. It has information about which conda environment was used to train the model, and what environment should be used to deploy the model. The file also specifies what version of Python should be used.\n",
    "* `score.py`: This script contains the `load_model()` and `predict()` functions. The `load_model()` function understands the format the model file was saved in and loads it into memory. The `.predict()` method is used to make inferences in a deployed model. There are also hooks that allow you to perform operations before and after inference. You can modify this script to fit your specific needs.\n",
    "\n",
    "To create the model artifacts, you use the `.prepare()` method. There are a number of parameters that allow you to store model provenance information. In the next cell, the `conda_env` variable defines the slug of the conda environment that was used to train the model, and also the conda environment that should be used for deployment. Note that you can only pass in slugs to `inference_conda_env` or `training_conda_env` if it's a service environment. Otherwise, you must pass in the full path of the conda envvironment along with the python version through `inference_python_version` and `training_python_version`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df177809",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env = 'generalml_p37_cpu_v1'\n",
    "\n",
    "sklearn_model.prepare(\n",
    "    inference_conda_env=conda_env,\n",
    "    training_conda_env=conda_env,\n",
    "    use_case_type=UseCaseType.BINARY_CLASSIFICATION,\n",
    "    X_sample=x, #X_test_xgbc,\n",
    "    y_sample=y, #y_test_xgbc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40b48c",
   "metadata": {},
   "source": [
    "The next cell uses the `.summary_status()` method to show you that the prepare step finished, and what tasks were completed.\n",
    "\n",
    "Make sure you apply the following changes to the end of score.py in case you want the endpoint to return probability of detection along with binary classification results:\n",
    "\n",
    "`input = pre_inference(data, input_schema_path)`\n",
    "\n",
    "`yhat_bin = post_inference(model.predict(input))`\n",
    "\n",
    "`yhat_prob = post_inference(model.predict_proba(input))`\n",
    "\n",
    "`return {'prediction': yhat_bin,'Probability':yhat_prob}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca19efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.summary_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df09eea",
   "metadata": {},
   "source": [
    "The `.prepare()` method has created the following files. These files are fully functional. However, you can modify them to fit your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699179a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(artifact_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab644810",
   "metadata": {},
   "source": [
    "Once the artifacts have been created, there are a number of attributes in the `SklearnModel` object that provides metadata about the model. The `.runtime` attribute details the model deployment settings and model provenance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e53010",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.runtime_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d93874",
   "metadata": {},
   "source": [
    "The `.schema_input` attribute provides metadata on the features that were used to train the model. You can use this information to determine what data must be provided to make model inferences. Each feature in the model has a section that defines the dtype, feature type, name, and if it is required. The metadata also includes the summary statistics associated with the feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.schema_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52c478",
   "metadata": {},
   "source": [
    "The `.metadata_custom` attribute provides custom metadata that contains information on the category of the metadata, description, key, and value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.metadata_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4249a",
   "metadata": {},
   "source": [
    "The `.metadata_provenance` contains information about the code and training data that was used to create the model. This information is most useful when a Git repository is being used to manage the code for training the model. This is considered a best practice because it allows you to do things like reproduce a model, perform forensic on the model, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af511f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.metadata_provenance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b9381",
   "metadata": {},
   "source": [
    "The `.metadata_taxonomy` is a key-value store that has information about the classification or taxonomy of the model. This can include information such as the model framework, use case type, hyperparameters, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.metadata_taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84654e93",
   "metadata": {},
   "source": [
    "<a id='serialize_verify'></a>\n",
    "## Verify\n",
    "\n",
    "If you modify the `score.py` file that is part of the model artifacts, then you should verify it. The verify step allows you to test those changes without having to deploy the model. This allows you to debug your code without having to save the model to the model catalog and then deploy it. The `.verify()` method takes a set of test parameters and performs the prediction by calling the `predict` function in `score.py`. It also runs the `load_model` function.\n",
    "\n",
    "The next cell simulates a call to a deployed model without having to actually deploy the model. It passes in test values and returns the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65b3af0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4\n",
      "1    0\n",
      "2    0\n",
      "Name: classLabels, dtype: category\n",
      "Categories (6, int64): [0 < 1 < 2 < 3 < 4 < 5]\n"
     ]
    }
   ],
   "source": [
    "print(y[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05872c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.joblib from model directory /home/datascience/WorkSpace/RedBull-Racining-TimeToPit/md-rbc-Pipeline-model ...\n",
      "Model is successfully loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': [4, 0, 0],\n",
       " 'Probability': [[0.15191290958757628,\n",
       "   0.13233933898206415,\n",
       "   0.2639862968980391,\n",
       "   0.101198299538517,\n",
       "   0.31874421251227697,\n",
       "   0.03181894248152647],\n",
       "  [0.8540234621368268,\n",
       "   0.03911242206269802,\n",
       "   0.024631108778133178,\n",
       "   0.046012479548423364,\n",
       "   0.02890497749643132,\n",
       "   0.007315549977487436],\n",
       "  [0.855121303747298,\n",
       "   0.06584647934354748,\n",
       "   0.011322612079286658,\n",
       "   0.03558893200207922,\n",
       "   0.02930013379949643,\n",
       "   0.0028205390282921325]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model.verify(x[0:3].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d27d012",
   "metadata": {},
   "source": [
    "The `.summary_status()` method is updated to show that the verify step has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dafe19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.summary_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31882e80",
   "metadata": {},
   "source": [
    "<a id='serialize_save'></a>\n",
    "## Save\n",
    "\n",
    "Once you are satisfied with the performance of the model and have verified that the `score.py` file is working, you can save the model to the model catalog. You do this with the `.save()` method on a `SklearnModel` object. This bundles up the model artifact that you have created, and push it to the model catalog. It returns the model OCID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7101e0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.joblib from model directory /home/datascience/WorkSpace/RedBull-Racining-TimeToPit/md-rbc-Pipeline-model ...\n",
      "Model is successfully loaded.\n",
      "['score.py', 'model.joblib', 'input_schema.json', 'runtime.yaml', 'test_json_output.json', 'output_schema.json']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact:/tmp/saved_model_14fde10e-dbff-4adf-ae96-89981ac36da5.zip\n"
     ]
    }
   ],
   "source": [
    "model_id = sklearn_model.save(display_name='rbr_deployed_Model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67d25322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ocid1.datasciencemodel.oc1.phx.amaaaaaanif7xwiapytqmlt4zzfwwgj3rto7c74pygomfxtf5zvdmewrxrxa'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9c682",
   "metadata": {},
   "source": [
    "<a id='serialize_deploy'></a>\n",
    "## Deploy\n",
    "\n",
    "With the model in the model catalog, you can use the `.deploy()` method of an `SklearnModel` object to deploy the model. This method allows you to specify the attributes of the deployment such as the display name, description, instance type and count, the maximum bandwidth, and logging groups. The next cell deployd the model with the default settings, except for the custom display name. The `.deploy()` method returns a `ModelDeployment` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d28a853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 794 ms, sys: 61.7 ms, total: 856 ms\n",
      "Wall time: 13min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deploy = sklearn_model.deploy(display_name='rbr_deployed_Model_v1',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1905aa",
   "metadata": {},
   "source": [
    "After deployment, the `.summary_status()` method shows that the model is ACTIVE and the `predict()` method is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30d02030",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.summary_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3cc22",
   "metadata": {},
   "source": [
    "<a id='invoke_endpoint'></a>\n",
    "# Invoking the Endpoint\n",
    "\n",
    "#### Invoke deployed model\n",
    "1. load data from storage though for this demo we are using in memory data here.\n",
    "2. create JSON string\n",
    "3. Hit the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97f26792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "import requests\n",
    "import io\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "ads.set_auth(\"resource_principal\")\n",
    "auth = oci.auth.signers.get_resource_principals_signer()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## since data is already in the memory therefore we are not reloading it. We use X_test_xgbc transform a few rows to JSON and invoke the end point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9aed0",
   "metadata": {},
   "source": [
    "### Invoke "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5805f2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c86225d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': [4, 0, 0],\n",
       " 'Probability': [[0.15191290958757628,\n",
       "   0.13233933898206415,\n",
       "   0.2639862968980391,\n",
       "   0.101198299538517,\n",
       "   0.31874421251227697,\n",
       "   0.03181894248152647],\n",
       "  [0.8540234621368268,\n",
       "   0.03911242206269802,\n",
       "   0.024631108778133178,\n",
       "   0.046012479548423364,\n",
       "   0.02890497749643132,\n",
       "   0.007315549977487436],\n",
       "  [0.855121303747298,\n",
       "   0.06584647934354748,\n",
       "   0.011322612079286658,\n",
       "   0.03558893200207922,\n",
       "   0.02930013379949643,\n",
       "   0.0028205390282921325]]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## select a few records and create JOSN string\n",
    "## Invoke the endpoint\n",
    "\n",
    "\n",
    "input_data = x[0:3].to_json(orient='records')\n",
    "endpoint = 'https://modeldeployment.us-phoenix-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.phx.amaaaaaanif7xwiai3sau6ookwt3vy46dtu67cglvuoazopgx3tsd43645ea/predict'\n",
    "body = input_data \n",
    "\n",
    "body = {'data': input_data, 'data_type':\"pandas.core.frame.DataFrame\"}\n",
    "\n",
    "requests.post(endpoint, json=body, auth=auth).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf6c14",
   "metadata": {},
   "source": [
    "<a id='cleanup'></a>\n",
    "# Clean Up\n",
    "\n",
    "This notebook created a model deployment and a model. This section cleans up those resources. \n",
    "\n",
    "The model deployment must be deleted before the model can be deleted. You use the `.delete_deployment()` method on the `SklearnModel` object to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f33332",
   "metadata": {},
   "source": [
    "## Delete a Model Deployment\n",
    "The `odsc` SDK provides two methods for deleting a model deployment. The `ModelDeployer` class has a static method delete(). This method can also be accessed on a `ModelDeployer` object. For example, this notebook has a variable `deployer` which is a `ModelDeployer` object. A model deployment can be deleted with:\n",
    "\n",
    "deployer.delete(model_deployment_id=deployment_id)\n",
    "or equivalently,\n",
    "\n",
    "ModelDeployer.delete(model_deployment_id=deployment_id)\n",
    "If you have a `ModelDeployment` object, there is a `delete()` method that will delete the model that is associated with that object. The optional parameter `wait_for_completion` accepts a boolean and determines if the process is blocking or not.\n",
    "\n",
    "The next cell will use a `ModelDeployment` object to delete the model deployment that was created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e16480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.delete_deployment(wait_for_completion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b098446",
   "metadata": {},
   "source": [
    "When a model deployment is deleted, it deletes the load balancer instances associated with it. If logging was configured, it does not delete that log group or loggers. In addition, the model in the model catalog is not deleted. In the next cell, these resources will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "588dc968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the log group and logs\n",
    "# logging_client = oci.logging.LoggingManagementClient(config=oci.config.from_file())\n",
    "# logging_client.delete_log(log_group_ocid, access_log_ocid)\n",
    "# logging_client.delete_log(log_group_ocid, predict_log_ocid)\n",
    "# logging_client.delete_log_group(log_group_ocid)\n",
    "\n",
    "# Delete the model\n",
    "ModelCatalog().delete_model(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffe441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalml_p37_cpu_v1]",
   "language": "python",
   "name": "conda-env-generalml_p37_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
